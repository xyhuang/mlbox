{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MLCube \u00b6 MLCube is a project that reduces friction for machine learning by ensuring that models are easily portable and reproducible, e.g., between different stacks such as different clouds, between cloud and on-prem, etc. Note: This project is still under development and some parts probably don't work yet, or may have unexpected/inconsistent behaviours. Installing MLCube \u00b6 Install from PyPI: pip install mlcube To uninstall: pip uninstall mlcube Usage Examples \u00b6 Check out the examples for detailed examples. License \u00b6 MLCube is licensed under the Apache License 2.0. See LICENSE for more information. Support \u00b6 Create a GitHub issue","title":"Home"},{"location":"#mlcube","text":"MLCube is a project that reduces friction for machine learning by ensuring that models are easily portable and reproducible, e.g., between different stacks such as different clouds, between cloud and on-prem, etc. Note: This project is still under development and some parts probably don't work yet, or may have unexpected/inconsistent behaviours.","title":"MLCube"},{"location":"#installing-mlcube","text":"Install from PyPI: pip install mlcube To uninstall: pip uninstall mlcube","title":"Installing MLCube"},{"location":"#usage-examples","text":"Check out the examples for detailed examples.","title":"Usage Examples"},{"location":"#license","text":"MLCube is licensed under the Apache License 2.0. See LICENSE for more information.","title":"License"},{"location":"#support","text":"Create a GitHub issue","title":"Support"},{"location":"getting-started/","text":"Installation \u00b6 Here is the step by step guide to run simple MLCube cubes. Create a python environment \u00b6 # Create Python Virtual Environment virtualenv -p python3 ./env && source ./env/bin/activate Install MLCube Runners \u00b6 # Install MLCube Docker runner pip install mlcube-docker # Optionally, setup host environment by providing the correct `http_proxy` and `https_proxy` environmental variables. # export http_proxy=... # export https_proxy=.. # Optionally, install other runners # pip install mlcube-singularity # pip install mlcube-ssh # pip install mlcube-k8s Explore with examples \u00b6 git clone https://github.com/mlperf/mlcube_examples.git && cd ./mlcube_examples A great way to learn about MLCube is try out the example MLCube cubes located here .","title":"Installation"},{"location":"getting-started/#installation","text":"Here is the step by step guide to run simple MLCube cubes.","title":"Installation"},{"location":"getting-started/#create-a-python-environment","text":"# Create Python Virtual Environment virtualenv -p python3 ./env && source ./env/bin/activate","title":"Create a python environment"},{"location":"getting-started/#install-mlcube-runners","text":"# Install MLCube Docker runner pip install mlcube-docker # Optionally, setup host environment by providing the correct `http_proxy` and `https_proxy` environmental variables. # export http_proxy=... # export https_proxy=.. # Optionally, install other runners # pip install mlcube-singularity # pip install mlcube-ssh # pip install mlcube-k8s","title":"Install MLCube Runners"},{"location":"getting-started/#explore-with-examples","text":"git clone https://github.com/mlperf/mlcube_examples.git && cd ./mlcube_examples A great way to learn about MLCube is try out the example MLCube cubes located here .","title":"Explore with examples"},{"location":"getting-started/hello-world/","text":"Hello World \u00b6 Hello World MLCube is an example of a docker-based cube. QuickStart \u00b6 Get started with MLCube Docker runner with below commands. Create python environment \u00b6 virtualenv -p python3 ./env && source ./env/bin/activate Install MLCube Docker runner \u00b6 pip install mlcube-docker Run Hello World MLCube example \u00b6 # hello_world MLCube is present in mlcube_examples repo. git clone https://github.com/mlperf/mlcube_examples.git && cd ./mlcube_examples/hello_world Run Hello World MLCube on a local machine with Docker runner # Configure Hello World MLCube mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml # Run Hello World training tasks: download data and train the model mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/hello.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/bye.yaml Setup Docker \u00b6 MLCube Docker runner used Docker runtime and they must be available in the system. Installation guides for various operating systems can be found here . This example was tested on a system where users are in the docker group and run docker without sudo . To add yourself to a docker group, run the following: sudo groupadd docker # Add the docker group if it doesn't already exist. sudo gpasswd -a ${USER} docker # Add the connected user \"${USER}\" to the docker group. Change the user name to match your preferred user. sudo service docker restart # Restart the Docker daemon. newgrp docker # Either do a `newgrp docker` or log out/in to activate the changes to groups. Configuring Hello World MLCube \u00b6 Cubes need to be configured before they can run. To do so, users need to run a MLCube runner with configure command providing path to a cube root directory and path to a platform configuration file. The Hello World cube is a docker-based cube, so users provide path to a MLCube Docker platform configuration file that sets a number of parameters, including docker image name: mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml The Docker runner will build a docker image for the Hello World cube. In general, this step is optional and is only required when MLCube needs to be rebuild. This can happen when users change implementation files and want to re-package their ML project into MLCube. In other situations, MLCube runners can auto-detect if configure command needs to be run before running a MLCube task. Running Hello World MLCube \u00b6 In order to run the Hello World cube, users need to provide the path to the root directory of the cube, platform configuration file and path to a task definition file. Run the following two commands one at a time: mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/hello.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/bye.yaml Hello World creates a file workspace/chats/chat_with_alice.txt that contains the following: [2020-09-03 09:13:14.236945] Hi, Alice! Nice to meet you. [2020-09-03 09:13:20.749831] Bye, Alice! It was great talking to you. Modifying MLCube tasks \u00b6 Adding new user \u00b6 Create a new file workspace/names/foo.txt with the following content: Foo . Create a new file run/foo/hello.yaml with the following content: schema_type : mlcube_invoke schema_version : 1.0.0 task_name : hello input_binding : name : $WORKSPACE/names/foo.txt output_binding : chat : $WORKSPACE/chats/chat_with_foo.txt Create a new file run/foo/bye.yaml with the following content: schema_type : mlcube_invoke schema_version : 1.0.0 task_name : bye input_binding : name : $WORKSPACE/names/foo.txt output_binding : chat : $WORKSPACE/chats/chat_with_foo.txt Run the following two commands one at a time: mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/foo/hello.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/foo/bye.yaml The Hello World cube creates a file workspace/chats/chat_with_foo.txt that contains the following: [2020-09-03 09:23:09.569558] Hi, Foo! Nice to meet you. [2020-09-03 09:23:20.076845] Bye, Foo! It was great talking to you. Providing a better greeting message \u00b6 Because how Hello World cube was implemented, the greeting message is always the following: Nice to meet you. . We will update the implementation so that if this is not the first time Alice says hello , the MLCube will respond: Nice to see you again. . Modify the file build/hello_world.py . Update the function named get_greeting_message on line 14. It should have the following implementation: def get_greeting_message ( chat_file : str ) -> str : return \"Nice to meet you.\" if not os . path . exists ( chat_file ) else \"Nice to see you again.\" Since we updated a file in the build subdirectory, we need to re-configure the Hello World cube: mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml Now, run two hello task again: mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/hello.yaml The MLCube recognized it was not the first time it talked to Alice, and changed the greeting: [2020-09-03 09:13:14.236945] Hi, Alice! Nice to meet you. [2020-09-03 09:13:20.749831] Bye, Alice! It was great talking to you. [2020-09-03 09:32:41.369367] Hi, Alice! Nice to see you again.","title":"Hello World"},{"location":"getting-started/hello-world/#hello-world","text":"Hello World MLCube is an example of a docker-based cube.","title":"Hello World"},{"location":"getting-started/hello-world/#quickstart","text":"Get started with MLCube Docker runner with below commands.","title":"QuickStart"},{"location":"getting-started/hello-world/#create-python-environment","text":"virtualenv -p python3 ./env && source ./env/bin/activate","title":"Create python environment"},{"location":"getting-started/hello-world/#install-mlcube-docker-runner","text":"pip install mlcube-docker","title":"Install MLCube Docker runner"},{"location":"getting-started/hello-world/#run-hello-world-mlcube-example","text":"# hello_world MLCube is present in mlcube_examples repo. git clone https://github.com/mlperf/mlcube_examples.git && cd ./mlcube_examples/hello_world Run Hello World MLCube on a local machine with Docker runner # Configure Hello World MLCube mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml # Run Hello World training tasks: download data and train the model mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/hello.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/bye.yaml","title":"Run Hello World MLCube example"},{"location":"getting-started/hello-world/#setup-docker","text":"MLCube Docker runner used Docker runtime and they must be available in the system. Installation guides for various operating systems can be found here . This example was tested on a system where users are in the docker group and run docker without sudo . To add yourself to a docker group, run the following: sudo groupadd docker # Add the docker group if it doesn't already exist. sudo gpasswd -a ${USER} docker # Add the connected user \"${USER}\" to the docker group. Change the user name to match your preferred user. sudo service docker restart # Restart the Docker daemon. newgrp docker # Either do a `newgrp docker` or log out/in to activate the changes to groups.","title":"Setup Docker"},{"location":"getting-started/hello-world/#configuring-hello-world-mlcube","text":"Cubes need to be configured before they can run. To do so, users need to run a MLCube runner with configure command providing path to a cube root directory and path to a platform configuration file. The Hello World cube is a docker-based cube, so users provide path to a MLCube Docker platform configuration file that sets a number of parameters, including docker image name: mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml The Docker runner will build a docker image for the Hello World cube. In general, this step is optional and is only required when MLCube needs to be rebuild. This can happen when users change implementation files and want to re-package their ML project into MLCube. In other situations, MLCube runners can auto-detect if configure command needs to be run before running a MLCube task.","title":"Configuring Hello World MLCube"},{"location":"getting-started/hello-world/#running-hello-world-mlcube","text":"In order to run the Hello World cube, users need to provide the path to the root directory of the cube, platform configuration file and path to a task definition file. Run the following two commands one at a time: mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/hello.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/bye.yaml Hello World creates a file workspace/chats/chat_with_alice.txt that contains the following: [2020-09-03 09:13:14.236945] Hi, Alice! Nice to meet you. [2020-09-03 09:13:20.749831] Bye, Alice! It was great talking to you.","title":"Running Hello World MLCube"},{"location":"getting-started/hello-world/#modifying-mlcube-tasks","text":"","title":"Modifying MLCube tasks"},{"location":"getting-started/hello-world/#adding-new-user","text":"Create a new file workspace/names/foo.txt with the following content: Foo . Create a new file run/foo/hello.yaml with the following content: schema_type : mlcube_invoke schema_version : 1.0.0 task_name : hello input_binding : name : $WORKSPACE/names/foo.txt output_binding : chat : $WORKSPACE/chats/chat_with_foo.txt Create a new file run/foo/bye.yaml with the following content: schema_type : mlcube_invoke schema_version : 1.0.0 task_name : bye input_binding : name : $WORKSPACE/names/foo.txt output_binding : chat : $WORKSPACE/chats/chat_with_foo.txt Run the following two commands one at a time: mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/foo/hello.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/foo/bye.yaml The Hello World cube creates a file workspace/chats/chat_with_foo.txt that contains the following: [2020-09-03 09:23:09.569558] Hi, Foo! Nice to meet you. [2020-09-03 09:23:20.076845] Bye, Foo! It was great talking to you.","title":"Adding new user"},{"location":"getting-started/hello-world/#providing-a-better-greeting-message","text":"Because how Hello World cube was implemented, the greeting message is always the following: Nice to meet you. . We will update the implementation so that if this is not the first time Alice says hello , the MLCube will respond: Nice to see you again. . Modify the file build/hello_world.py . Update the function named get_greeting_message on line 14. It should have the following implementation: def get_greeting_message ( chat_file : str ) -> str : return \"Nice to meet you.\" if not os . path . exists ( chat_file ) else \"Nice to see you again.\" Since we updated a file in the build subdirectory, we need to re-configure the Hello World cube: mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml Now, run two hello task again: mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/alice/hello.yaml The MLCube recognized it was not the first time it talked to Alice, and changed the greeting: [2020-09-03 09:13:14.236945] Hi, Alice! Nice to meet you. [2020-09-03 09:13:20.749831] Bye, Alice! It was great talking to you. [2020-09-03 09:32:41.369367] Hi, Alice! Nice to see you again.","title":"Providing a better greeting message"},{"location":"getting-started/mnist/","text":"MNIST \u00b6 The MNIST dataset is a collection of 60,000 handwritten digits widely used for training statistical, Machine Learning (ML) and Deep Learning (DL) models. The MNIST MLCube example demonstrates how data scientists, ML and DL researchers and developers can distribute their ML projects (including training, validation and inference code) as MLCube cubes. MLCube establishes a standard to package user workloads, and provides unified command line interface. In addition, MLCube provides a number of reference runners - python packages that can run cubes on different platforms including docker and singularity. A data scientist has been working on a machine learning project. The goal is to train a simple neural network to classify collection of 60,000 small images into 10 classes. MNIST training code \u00b6 Training a ML model is a process involving multiple steps such as getting data, analyzing and cleaning data, splitting into train/validation/test data sets, running hyper-parameter optimization experiments and performing final model testing. It is a relatively small and well studied dataset that provides standard train/test split. In this simple example a developer needs to implement two steps - (1) downloading data and (2) training a model. We'll call these steps as tasks . Each task requires several parameters, such as URL of the data set that we need to download, location on a local disk where the data set will be serialized, path to a directory that will contain training artifacts such as log files, training snapshots and ML models. We can characterize these two tasks in the following way: - Data Download task: - Inputs : None. We'll assume the download URL is defined in the source code. - Outputs : Directory to serialize the data set ( data_dir ) and directory to serialize log files ( log_dir ). - Training task: - Inputs : Directory with MNIST data set ( data_dir ), training hyper-parameters defined in a file ( parameters_file ). - Outputs : Directory to store training results ( model_dir ) and directory to store log files ( log_dir ). We have intentionally made all input/output parameters to be file system artifacts. By doing so, we support reproducibility. Instead of command line arguments that can easily be lost, we store them in files. There are many different ways to implement the MNIST example. For simplicity, we assume the following: - We use one python file. - Task name (download, train) is a command line positional parameter. - Both tasks write logs, so it makes sense to add parameter accepting directory for log files. - The download task accepts additional data directory parameter. - The train task accepts such parameters as data and model directories, path to a file with hyper-parameter. - Configurable hyper-parameters are: (1) optimizer name, (2) number of training epochs and (3) global batch size. Then, our implementation could look like this. Parse command line and identify task. If it is download , call a function that downloads data sets. If it is train , train a model. This is sort of single entrypoint implementation where we run one script asking to perform various tasks. We run our script (mnist.py) in the following way: python mnist.py download --data_dir=PATH --log_dir=PATH python mnist.py train --data_dir=PATH --log_dir=PATH --model_dir=PATH --parameters_file=PATH MLCube implementation \u00b6 Packaging our MNIST training script as a MLCube is done in several steps. We will be using a directory-based cube where a directory is structured in a certain way and contains specific files that make it MLCube compliant. We need to create an empty directory on a local disk. Let's assume we call it mnist and we'll use {MLCUBE_ROOT} to denote a full path to this directory. This is called a cube root directory. At this point this directory is empty: mnist/ Build location \u00b6 The cube directory has a sub-directory called build ( {MLCUBE_ROOT}/build ) that stores project source files, resources required for training, other files to recreate run time (such as requirements.txt, docker and singularity recipes etc.). We need to create the build directory and copy two files: mnist.py that implements training and requirements.txt that lists python dependencies. By doing so, we are enforcing reproducibility. A developer of this cube wants to make it easier to run their training workload in a great variety of environments including universities, commercial companies, HPC-friendly organizations such as national labs. One way to achieve it is to use container runtime such as docker or singularity. So, we'll provide both docker file and singularity recipe that we'll put into build directory as well. Thus, we'll make this directory a build context. The cube directory now looks like: mnist/ build/ mnist.py requirements.txt Dockerfile Singularity.recipe A good test at this point would be ensure that project is runnable from the build directory, and docker and singularity images can be built. MLCube definition file \u00b6 At this point we are ready to create a cube definition file. This is the first definition file that makes some folder a MLCube folder. This is a YAML file that provides information such as name, author, version, named as mlcube.yaml and located in the cube root directory . The most important section is the one that lists what tasks are implemented in this cube: schema_version : 1.0.0 # We use MLSpec library to validate YAML definition files. This is the schema_type : mlcube_root # specification of the schema that this file must be consistent with. name : mnist # Name of this cube. author : MLPerf Best Practices Working Group # A developer of the cube. version : 0.1.0 # MLCube version. mlcube_spec_version : 0.1.0 # TODO: What is it? tasks : # Tasks are defined in external YAML files located in tasks folder. - 'tasks/download.yaml' # \"Download data set\" task definition file. - 'tasks/train.yaml' # \"Training a model\" task definition file. At this point, the directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} mlcube.yaml Task definition file \u00b6 The cube definition file references two tasks defined in the tasks subdirectory. Each YAML file there defines a task supported by the cube. Task files are named the same as tasks. We need to create a tasks directory and two files inside that directory - download.yaml and train.yaml . Each task file defines input and output specifications for each task. The download task (download.yaml) is defined: schema_version : 1.0.0 # Task schema definition. Leave this two fields as is. schema_type : mlcube_task inputs : [] # Since this task does not have any inputs, the section is empty. outputs : # This task produces two artifacts - downloaded data and log files. - name : data_dir # This parameter accepts path to a directory where data set will be serialized. type : directory # We implicitly specify that this is a directory - name : log_dir # This parameter accepts path to a directory with log files this task writes. type : directory # We implicitly specify that this is a directory Names of these parameters are the same that are accepted by mnist.py: python mnist.py download --data_dir=PATH --log_dir=PATH The train task ( train.yaml ) is defined in the following way: schema_version : 1.0.0 # Task schema definition. Leave this two fields as is. schema_type : mlcube_task inputs : # These are the task inputs. - name : data_dir # This parameter accepts path to a directory where data set will be serialized. type : directory # We implicitly specify that this is a directory - name : parameters_file # A file containing training hyper-parameters. type : file # This is a file. outputs : # These are the task outputs. - name : log_dir # This parameter accepts path to a directory with log files this task writes. type : directory # We implicitly specify that this is a directory - name : model_dir # Path to a directory where training artifacts are stored. type : directory # We implicitly specify that this is a directory Names of these parameters are the same that are accepted by mnist.py: python mnist.py train --data_dir=PATH --log_dir=PATH --model_dir=PATH --parameters_file=PATH At this point, the MLCube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} mlcube.yaml Workspace \u00b6 The workspace is a directory inside cube ( workspace ) where, by default, input/output file system artifacts are stored. The are multiple reasons to have one. One is to formally have default place for data sets, configuration and log files etc. Having all these parameters in one place makes it simpler to run cubes on remote hosts and then sync results back to users' local machines. We need to be able to provide collection of hyper-parameters and formally define a directory to store logs, models and MNIST data set. To do so, we create the directory tree workspace/parameters , and then create a file ( default.parameters.yaml ) with the following content: optimizer : \"adam\" train_epochs : 5 batch_size : 32 At this point, the cube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} workspace/ parameters/ default.parameters.yaml mlcube.yaml Run configurations \u00b6 The MLCube definition file ( mlcube.yaml ) provides paths to task definition files that formally define tasks input/output parameters. A run configuration assigns values to task parameters. One reason to define and \"implement\" parameters in different files is to be able to provide multiple configurations for the same task. One example could be one-GPU training configuration and 8-GPU training configuration. Since we have two tasks - download and train - we need to define at least two run configurations. Run configurations are defined in the run subdirectory. Run configuration for the download task looks like: schema_type : mlcube_invoke # Run (invoke) schema definition. Leave this two fields as is. schema_version : 1.0.0 task_name : download # Task name input_binding : {} # No input parameters for this task. output_binding : # Output parameters, format is \"parameter: value\" data_dir : $WORKSPACE/data # Path to serialize downloaded MNIST data set log_dir : $WORKSPACE/download_logs # Path to log files. The $WORKSPACE token is replaced with actual path to the cube workspace. File system paths are relative to the workspace directory. This makes it possible to provide absolute paths for cases when data sets are stored on shared drives. Run configuration for the train task looks like: schema_type : mlcube_invoke # Run (invoke) schema definition. Leave this two fields as is. schema_version : 1.0.0 task_name : train # Task name input_binding : # Input parameters (name: value) data_dir : $WORKSPACE/data parameters_file : $WORKSPACE/parameters/default.parameters.yaml output_binding : # Output parameters (name: value) log_dir : $WORKSPACE/train_logs model_dir : $WORKSPACE/model At this point, the cube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} workspace/parameters/default.parameters.yaml run/ download.yaml train.yaml mlcube.yaml Platform configurations \u00b6 Platform configurations define how MLCube cubes run. Docker, Singularity, SSH and cloud runners have their own configurations. For instance, Docker platform configuration at minimum provides image name and docker executable (docker / nvidia-docker). SSH platform configuration could provide IP address of a remote host, login credentials etc. Platform configurations are supposed to be used by runners, and each runner has its own platform schema. The Runners documentation section provides detailed description of reference runners together with platform configuration schemas. Since we wanted to support Docker and Singularity runtimes, we provide docker.yaml and singularity.yaml files in the platforms subdirectory that is default location to store these types of files. Docker platform configuration is the following: schema_version : 1.0.0 schema_type : mlcube_docker image : mlperf/mlcube:mnist # Docker image name docker_runtime : docker # Docker executable: docker or nvidia-docker Singularity platform configuration is the following: schema_version : 1.0.0 schema_type : mlcube_singularity image : /opt/singularity/mlperf_mlcube_mnist-0.01.simg # Path to or name of a Singularity image. At this point, the cube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} workspace/parameters/default.parameters.yaml run/ {download.yaml, train.yaml} platforms/ docker.yaml singularity.yaml mlcube.yaml MNIST MLCube directory structure summary \u00b6 mnist/ # MLCube root directory. build/ # Project source code, resource files, Docker/Singularity recipes. mnist.py # Python source code training simple neural network using MNIST data set. requirements.txt # Python project dependencies. Dockerfile # Docker recipe. Singularity.recipe # Singularity recipe. tasks/ # Task definition files - define functionality that MLCube supports download.yaml # Download MNIST data set. train.yaml # Train neural network. workspace/ # Default location for data sets, logs, models, parameter files. parameters/ # Model hyper-parameters can be stored at any location. default.parameters.yaml # This is just what is used in this implementation. run/ # Run configurations - bind task parameters and values. download.yaml # Concrete run specification for the download task. train.yaml # Concrete run specification for the train task. platforms/ # Platform definition files - define how MLCube runs. docker.yaml # Docker runtime definition. singularity.yaml # Singularity runtime definition. mlcube.yaml # MLCube definition file. Running MNIST MLCube \u00b6 We need to setup the Python virtual environment. These are the steps outlined in the Introduction section except we do not clone GitHub repository with the example MLCube cubes. # Create Python Virtual Environment virtualenv -p python3 ./env && source ./env/bin/activate # Install MLCube Docker and Singularity runners pip install mlcube-docker mlcube-singularity # Optionally, setup host environment by providing the correct `http_proxy` and `https_proxy` environmental variables. # export http_proxy=... # export https_proxy=.. Before running MNIST cube below, it is probably a good idea to remove tasks' outputs from previous runs that are located in the workspace directory. All directories except parameters can be removed. Docker Runner \u00b6 Configure MNIST cube (this is optional step, docker runner checks if image exists, and if does not, runs configure phase automatically): mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml Run two tasks - download (download data) and train (train tiny neural network): mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/download.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/train.yaml Singularity Runner \u00b6 Update path to store Singularity image. Open platforms/singularity.yaml and update the image value that is set by default to /opt/singularity/mlperf_mlcube_mnist-0.01.simg (relative paths are supported, they are relative to workspace ). Configure MNIST cube: mlcube_singularity configure --mlcube=. --platform=platforms/singularity.yaml Run two tasks - download (download data) and train (train tiny neural network): mlcube_singularity run --mlcube=. --platform=platforms/singularity.yaml --task=run/download.yaml mlcube_singularity run --mlcube=. --platform=platforms/singularity.yaml --task=run/train.yaml","title":"MNIST"},{"location":"getting-started/mnist/#mnist","text":"The MNIST dataset is a collection of 60,000 handwritten digits widely used for training statistical, Machine Learning (ML) and Deep Learning (DL) models. The MNIST MLCube example demonstrates how data scientists, ML and DL researchers and developers can distribute their ML projects (including training, validation and inference code) as MLCube cubes. MLCube establishes a standard to package user workloads, and provides unified command line interface. In addition, MLCube provides a number of reference runners - python packages that can run cubes on different platforms including docker and singularity. A data scientist has been working on a machine learning project. The goal is to train a simple neural network to classify collection of 60,000 small images into 10 classes.","title":"MNIST"},{"location":"getting-started/mnist/#mnist-training-code","text":"Training a ML model is a process involving multiple steps such as getting data, analyzing and cleaning data, splitting into train/validation/test data sets, running hyper-parameter optimization experiments and performing final model testing. It is a relatively small and well studied dataset that provides standard train/test split. In this simple example a developer needs to implement two steps - (1) downloading data and (2) training a model. We'll call these steps as tasks . Each task requires several parameters, such as URL of the data set that we need to download, location on a local disk where the data set will be serialized, path to a directory that will contain training artifacts such as log files, training snapshots and ML models. We can characterize these two tasks in the following way: - Data Download task: - Inputs : None. We'll assume the download URL is defined in the source code. - Outputs : Directory to serialize the data set ( data_dir ) and directory to serialize log files ( log_dir ). - Training task: - Inputs : Directory with MNIST data set ( data_dir ), training hyper-parameters defined in a file ( parameters_file ). - Outputs : Directory to store training results ( model_dir ) and directory to store log files ( log_dir ). We have intentionally made all input/output parameters to be file system artifacts. By doing so, we support reproducibility. Instead of command line arguments that can easily be lost, we store them in files. There are many different ways to implement the MNIST example. For simplicity, we assume the following: - We use one python file. - Task name (download, train) is a command line positional parameter. - Both tasks write logs, so it makes sense to add parameter accepting directory for log files. - The download task accepts additional data directory parameter. - The train task accepts such parameters as data and model directories, path to a file with hyper-parameter. - Configurable hyper-parameters are: (1) optimizer name, (2) number of training epochs and (3) global batch size. Then, our implementation could look like this. Parse command line and identify task. If it is download , call a function that downloads data sets. If it is train , train a model. This is sort of single entrypoint implementation where we run one script asking to perform various tasks. We run our script (mnist.py) in the following way: python mnist.py download --data_dir=PATH --log_dir=PATH python mnist.py train --data_dir=PATH --log_dir=PATH --model_dir=PATH --parameters_file=PATH","title":"MNIST training code"},{"location":"getting-started/mnist/#mlcube-implementation","text":"Packaging our MNIST training script as a MLCube is done in several steps. We will be using a directory-based cube where a directory is structured in a certain way and contains specific files that make it MLCube compliant. We need to create an empty directory on a local disk. Let's assume we call it mnist and we'll use {MLCUBE_ROOT} to denote a full path to this directory. This is called a cube root directory. At this point this directory is empty: mnist/","title":"MLCube implementation"},{"location":"getting-started/mnist/#build-location","text":"The cube directory has a sub-directory called build ( {MLCUBE_ROOT}/build ) that stores project source files, resources required for training, other files to recreate run time (such as requirements.txt, docker and singularity recipes etc.). We need to create the build directory and copy two files: mnist.py that implements training and requirements.txt that lists python dependencies. By doing so, we are enforcing reproducibility. A developer of this cube wants to make it easier to run their training workload in a great variety of environments including universities, commercial companies, HPC-friendly organizations such as national labs. One way to achieve it is to use container runtime such as docker or singularity. So, we'll provide both docker file and singularity recipe that we'll put into build directory as well. Thus, we'll make this directory a build context. The cube directory now looks like: mnist/ build/ mnist.py requirements.txt Dockerfile Singularity.recipe A good test at this point would be ensure that project is runnable from the build directory, and docker and singularity images can be built.","title":"Build location"},{"location":"getting-started/mnist/#mlcube-definition-file","text":"At this point we are ready to create a cube definition file. This is the first definition file that makes some folder a MLCube folder. This is a YAML file that provides information such as name, author, version, named as mlcube.yaml and located in the cube root directory . The most important section is the one that lists what tasks are implemented in this cube: schema_version : 1.0.0 # We use MLSpec library to validate YAML definition files. This is the schema_type : mlcube_root # specification of the schema that this file must be consistent with. name : mnist # Name of this cube. author : MLPerf Best Practices Working Group # A developer of the cube. version : 0.1.0 # MLCube version. mlcube_spec_version : 0.1.0 # TODO: What is it? tasks : # Tasks are defined in external YAML files located in tasks folder. - 'tasks/download.yaml' # \"Download data set\" task definition file. - 'tasks/train.yaml' # \"Training a model\" task definition file. At this point, the directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} mlcube.yaml","title":"MLCube definition file"},{"location":"getting-started/mnist/#task-definition-file","text":"The cube definition file references two tasks defined in the tasks subdirectory. Each YAML file there defines a task supported by the cube. Task files are named the same as tasks. We need to create a tasks directory and two files inside that directory - download.yaml and train.yaml . Each task file defines input and output specifications for each task. The download task (download.yaml) is defined: schema_version : 1.0.0 # Task schema definition. Leave this two fields as is. schema_type : mlcube_task inputs : [] # Since this task does not have any inputs, the section is empty. outputs : # This task produces two artifacts - downloaded data and log files. - name : data_dir # This parameter accepts path to a directory where data set will be serialized. type : directory # We implicitly specify that this is a directory - name : log_dir # This parameter accepts path to a directory with log files this task writes. type : directory # We implicitly specify that this is a directory Names of these parameters are the same that are accepted by mnist.py: python mnist.py download --data_dir=PATH --log_dir=PATH The train task ( train.yaml ) is defined in the following way: schema_version : 1.0.0 # Task schema definition. Leave this two fields as is. schema_type : mlcube_task inputs : # These are the task inputs. - name : data_dir # This parameter accepts path to a directory where data set will be serialized. type : directory # We implicitly specify that this is a directory - name : parameters_file # A file containing training hyper-parameters. type : file # This is a file. outputs : # These are the task outputs. - name : log_dir # This parameter accepts path to a directory with log files this task writes. type : directory # We implicitly specify that this is a directory - name : model_dir # Path to a directory where training artifacts are stored. type : directory # We implicitly specify that this is a directory Names of these parameters are the same that are accepted by mnist.py: python mnist.py train --data_dir=PATH --log_dir=PATH --model_dir=PATH --parameters_file=PATH At this point, the MLCube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} mlcube.yaml","title":"Task definition file"},{"location":"getting-started/mnist/#workspace","text":"The workspace is a directory inside cube ( workspace ) where, by default, input/output file system artifacts are stored. The are multiple reasons to have one. One is to formally have default place for data sets, configuration and log files etc. Having all these parameters in one place makes it simpler to run cubes on remote hosts and then sync results back to users' local machines. We need to be able to provide collection of hyper-parameters and formally define a directory to store logs, models and MNIST data set. To do so, we create the directory tree workspace/parameters , and then create a file ( default.parameters.yaml ) with the following content: optimizer : \"adam\" train_epochs : 5 batch_size : 32 At this point, the cube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} workspace/ parameters/ default.parameters.yaml mlcube.yaml","title":"Workspace"},{"location":"getting-started/mnist/#run-configurations","text":"The MLCube definition file ( mlcube.yaml ) provides paths to task definition files that formally define tasks input/output parameters. A run configuration assigns values to task parameters. One reason to define and \"implement\" parameters in different files is to be able to provide multiple configurations for the same task. One example could be one-GPU training configuration and 8-GPU training configuration. Since we have two tasks - download and train - we need to define at least two run configurations. Run configurations are defined in the run subdirectory. Run configuration for the download task looks like: schema_type : mlcube_invoke # Run (invoke) schema definition. Leave this two fields as is. schema_version : 1.0.0 task_name : download # Task name input_binding : {} # No input parameters for this task. output_binding : # Output parameters, format is \"parameter: value\" data_dir : $WORKSPACE/data # Path to serialize downloaded MNIST data set log_dir : $WORKSPACE/download_logs # Path to log files. The $WORKSPACE token is replaced with actual path to the cube workspace. File system paths are relative to the workspace directory. This makes it possible to provide absolute paths for cases when data sets are stored on shared drives. Run configuration for the train task looks like: schema_type : mlcube_invoke # Run (invoke) schema definition. Leave this two fields as is. schema_version : 1.0.0 task_name : train # Task name input_binding : # Input parameters (name: value) data_dir : $WORKSPACE/data parameters_file : $WORKSPACE/parameters/default.parameters.yaml output_binding : # Output parameters (name: value) log_dir : $WORKSPACE/train_logs model_dir : $WORKSPACE/model At this point, the cube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} workspace/parameters/default.parameters.yaml run/ download.yaml train.yaml mlcube.yaml","title":"Run configurations"},{"location":"getting-started/mnist/#platform-configurations","text":"Platform configurations define how MLCube cubes run. Docker, Singularity, SSH and cloud runners have their own configurations. For instance, Docker platform configuration at minimum provides image name and docker executable (docker / nvidia-docker). SSH platform configuration could provide IP address of a remote host, login credentials etc. Platform configurations are supposed to be used by runners, and each runner has its own platform schema. The Runners documentation section provides detailed description of reference runners together with platform configuration schemas. Since we wanted to support Docker and Singularity runtimes, we provide docker.yaml and singularity.yaml files in the platforms subdirectory that is default location to store these types of files. Docker platform configuration is the following: schema_version : 1.0.0 schema_type : mlcube_docker image : mlperf/mlcube:mnist # Docker image name docker_runtime : docker # Docker executable: docker or nvidia-docker Singularity platform configuration is the following: schema_version : 1.0.0 schema_type : mlcube_singularity image : /opt/singularity/mlperf_mlcube_mnist-0.01.simg # Path to or name of a Singularity image. At this point, the cube directory looks like: mnist/ build/ {mnist.py, requirements.txt, Dockerfile, Singularity.recipe} tasks/ {download.yaml, train.yaml} workspace/parameters/default.parameters.yaml run/ {download.yaml, train.yaml} platforms/ docker.yaml singularity.yaml mlcube.yaml","title":"Platform configurations"},{"location":"getting-started/mnist/#mnist-mlcube-directory-structure-summary","text":"mnist/ # MLCube root directory. build/ # Project source code, resource files, Docker/Singularity recipes. mnist.py # Python source code training simple neural network using MNIST data set. requirements.txt # Python project dependencies. Dockerfile # Docker recipe. Singularity.recipe # Singularity recipe. tasks/ # Task definition files - define functionality that MLCube supports download.yaml # Download MNIST data set. train.yaml # Train neural network. workspace/ # Default location for data sets, logs, models, parameter files. parameters/ # Model hyper-parameters can be stored at any location. default.parameters.yaml # This is just what is used in this implementation. run/ # Run configurations - bind task parameters and values. download.yaml # Concrete run specification for the download task. train.yaml # Concrete run specification for the train task. platforms/ # Platform definition files - define how MLCube runs. docker.yaml # Docker runtime definition. singularity.yaml # Singularity runtime definition. mlcube.yaml # MLCube definition file.","title":"MNIST MLCube directory structure summary"},{"location":"getting-started/mnist/#running-mnist-mlcube","text":"We need to setup the Python virtual environment. These are the steps outlined in the Introduction section except we do not clone GitHub repository with the example MLCube cubes. # Create Python Virtual Environment virtualenv -p python3 ./env && source ./env/bin/activate # Install MLCube Docker and Singularity runners pip install mlcube-docker mlcube-singularity # Optionally, setup host environment by providing the correct `http_proxy` and `https_proxy` environmental variables. # export http_proxy=... # export https_proxy=.. Before running MNIST cube below, it is probably a good idea to remove tasks' outputs from previous runs that are located in the workspace directory. All directories except parameters can be removed.","title":"Running MNIST MLCube"},{"location":"getting-started/mnist/#docker-runner","text":"Configure MNIST cube (this is optional step, docker runner checks if image exists, and if does not, runs configure phase automatically): mlcube_docker configure --mlcube=. --platform=platforms/docker.yaml Run two tasks - download (download data) and train (train tiny neural network): mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/download.yaml mlcube_docker run --mlcube=. --platform=platforms/docker.yaml --task=run/train.yaml","title":"Docker Runner"},{"location":"getting-started/mnist/#singularity-runner","text":"Update path to store Singularity image. Open platforms/singularity.yaml and update the image value that is set by default to /opt/singularity/mlperf_mlcube_mnist-0.01.simg (relative paths are supported, they are relative to workspace ). Configure MNIST cube: mlcube_singularity configure --mlcube=. --platform=platforms/singularity.yaml Run two tasks - download (download data) and train (train tiny neural network): mlcube_singularity run --mlcube=. --platform=platforms/singularity.yaml --task=run/download.yaml mlcube_singularity run --mlcube=. --platform=platforms/singularity.yaml --task=run/train.yaml","title":"Singularity Runner"},{"location":"runners/","text":"Runners \u00b6 A runner is a tool that runs MLCube cubes on one or multiple platforms. Examples of platforms are docker and singularity containers, remote hosts, virtual machines in the cloud, etc. A platform is configured in a platform configuration file. A cube can provide reference platform configurations that users can modify to meet requirements of their infrastructures. MLCube standard requires that all runners implement mandatory functionality. All reference runners implement it. Users can develop their own runners to meet their specific requirements, such as security, authentication and authorization policies and others. Reference MLCube runners \u00b6 Reference runners are: - Docker Runner : Runs cubes using docker runtime. - Singularity Runner : Runs cubes using singularity runtime. - SSH Runner : Runs cubes on remote hosts. SSH Runner uses other runners, such as Docker or Singularity runners, to run cubes on remote hosts. Runner commands \u00b6 Each runner exposes mandatory and optional functionality through a set of commands. This is similar to, for instance, how Git implements its CLI ( git followed by a specific command such as checkout , pull , push etc). Mandatory MLCube runner commands are configure and run : - configure : Configure MLCube. Exact functionality depends on a runner type, but the goal is to ensure that a cube is ready to run. The following are the examples of what can be done at configure phase: build docker or singularity container, create python virtual environment, allocate and configure virtual machine in the cloud, copy cube to a remote host etc. Once configuration is successfully completed, it is assumed a runner can run that cube. - run : Run tasks defined in MLCube. Reference runners recognize three parameters - mlcube, platform and task. - mlcube : Path to a cube root directory. In future versions, this can be an URI with a specific protocol. Runners could support various MLCube implementations (excluding reference directory-based) such as docker/singularity containers, GitHub repositories, compressed archives and others. - platform : Path to a YAML-based platform configuration file. If not present, a runner should use the default platforms to run a cube, or select the most appropriate or available in a user environment. - task : Path to a YAML-based task specification file. If not present, a runner can run the default task. Command line interface \u00b6 One way to run a MLCube is to follow the following template supported by all reference runners: python -m RUNNER_PACKAGE --mlcube=MLCUBE_ROOT_DIRECTORY --platform=PLATFORM_FILE_PATH --task=TASK_FILE_PATH Example command to configure MNIST Docker-based MLCube: python -m mlcube_docker configure --mlcube=examples/mnist --platform=examples/mnist/platform/docker.yaml Example command to run two tasks implemented by the MNIST Docker-based MLCube: python -m mlcube_docker run --mlcube=examples/mnist --platform=examples/mnist/platform/docker.yaml --task=examples/mnist/run/download.yaml python -m mlcube_docker run --mlcube=examples/mnist --platform=examples/mnist/platform/docker.yaml --task=examples/mnist/run/train.yaml","title":"Runners"},{"location":"runners/#runners","text":"A runner is a tool that runs MLCube cubes on one or multiple platforms. Examples of platforms are docker and singularity containers, remote hosts, virtual machines in the cloud, etc. A platform is configured in a platform configuration file. A cube can provide reference platform configurations that users can modify to meet requirements of their infrastructures. MLCube standard requires that all runners implement mandatory functionality. All reference runners implement it. Users can develop their own runners to meet their specific requirements, such as security, authentication and authorization policies and others.","title":"Runners"},{"location":"runners/#reference-mlcube-runners","text":"Reference runners are: - Docker Runner : Runs cubes using docker runtime. - Singularity Runner : Runs cubes using singularity runtime. - SSH Runner : Runs cubes on remote hosts. SSH Runner uses other runners, such as Docker or Singularity runners, to run cubes on remote hosts.","title":"Reference MLCube runners"},{"location":"runners/#runner-commands","text":"Each runner exposes mandatory and optional functionality through a set of commands. This is similar to, for instance, how Git implements its CLI ( git followed by a specific command such as checkout , pull , push etc). Mandatory MLCube runner commands are configure and run : - configure : Configure MLCube. Exact functionality depends on a runner type, but the goal is to ensure that a cube is ready to run. The following are the examples of what can be done at configure phase: build docker or singularity container, create python virtual environment, allocate and configure virtual machine in the cloud, copy cube to a remote host etc. Once configuration is successfully completed, it is assumed a runner can run that cube. - run : Run tasks defined in MLCube. Reference runners recognize three parameters - mlcube, platform and task. - mlcube : Path to a cube root directory. In future versions, this can be an URI with a specific protocol. Runners could support various MLCube implementations (excluding reference directory-based) such as docker/singularity containers, GitHub repositories, compressed archives and others. - platform : Path to a YAML-based platform configuration file. If not present, a runner should use the default platforms to run a cube, or select the most appropriate or available in a user environment. - task : Path to a YAML-based task specification file. If not present, a runner can run the default task.","title":"Runner commands"},{"location":"runners/#command-line-interface","text":"One way to run a MLCube is to follow the following template supported by all reference runners: python -m RUNNER_PACKAGE --mlcube=MLCUBE_ROOT_DIRECTORY --platform=PLATFORM_FILE_PATH --task=TASK_FILE_PATH Example command to configure MNIST Docker-based MLCube: python -m mlcube_docker configure --mlcube=examples/mnist --platform=examples/mnist/platform/docker.yaml Example command to run two tasks implemented by the MNIST Docker-based MLCube: python -m mlcube_docker run --mlcube=examples/mnist --platform=examples/mnist/platform/docker.yaml --task=examples/mnist/run/download.yaml python -m mlcube_docker run --mlcube=examples/mnist --platform=examples/mnist/platform/docker.yaml --task=examples/mnist/run/train.yaml","title":"Command line interface"},{"location":"runners/docker-runner/","text":"Docker Runner \u00b6 Docker runner uses docker/nvidia-docker to run MLCube cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Docker platform configuration is used to configure docker runner. Platform Configuration File \u00b6 Docker platform configuration file is a YAML file that follows mlcube_docker ML schema. The configuration file for the reference MNIST cube is the following: schema_version : 1.0.0 schema_type : mlcube_docker image : mlperf/mlcube:mnist # Docker image name docker_runtime : docker # Docker executable: docker or nvidia-docker Additional configuration \u00b6 In current implementation, Docker runner uses http_proxy and https_proxy environmental variables (if set) during configure and run phases: - configure : docker build ... --build-args http_proxy=${http_proxy} --build-args https_proxy=${https_proxy} ... - run : docker run ... -e http_proxy=${http_proxy} -e https_proxy=${https_proxy} ... Configuring MLCubes \u00b6 Docker runner uses {MLCUBE_ROOT}/build directory as the build context directory. This implies that all files that must be packaged in a docker image, must be located in that directory, including source files, python requirements, resource files, ML models etc. The docker recipe must have the standard name Dockerfile . If Dockerfile file exists in {MLCUBE_ROOT}/build , the Docker runner assumes that it needs to build a docker image. If that file does not exists, the Docker runner will try to pull image with the specified name. Docker runner under the hood runs the following command line: cd {build_path}; docker build {env_args} -t {image_name} -f Dockerfile . where: - {build_path} is {MLCUBE_ROOT}/build root directory. - {env_args} is the arguments retrieved from user environment. Currently, only http_proxy and https_proxy are supported. - {image_name} is the image name defined in the platform configuration file. The configure command is optional and users do not necessarily need to be aware about it. The Docker runner auto-detects if docker image exists before running a task, and if it does not exist, the docker runner runs the configure command. During the configure phase, docker runner does not check if docker image exists. This means the following. If some of the implementation files have been modified, to rebuild the docker image users need to run the configure command explicitly. Running MLCubes \u00b6 Docker runner runs the following command: {docker_runtime} run --rm --net=host --privileged=true {volumes} {env_args} {image_name} {args} where: - {docker_exec} is the docker_runtime value from the Docker platform configuration file. - {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. - {env_args} is the arguments retrieved from user environment, currently, only http_proxy and https_proxy are supported. - {image_name} is the image name from the platform configuration file. - {args} is the task command line arguments, constructed automatically by the runner.","title":"Docker Runner"},{"location":"runners/docker-runner/#docker-runner","text":"Docker runner uses docker/nvidia-docker to run MLCube cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Docker platform configuration is used to configure docker runner.","title":"Docker Runner"},{"location":"runners/docker-runner/#platform-configuration-file","text":"Docker platform configuration file is a YAML file that follows mlcube_docker ML schema. The configuration file for the reference MNIST cube is the following: schema_version : 1.0.0 schema_type : mlcube_docker image : mlperf/mlcube:mnist # Docker image name docker_runtime : docker # Docker executable: docker or nvidia-docker","title":"Platform Configuration File"},{"location":"runners/docker-runner/#additional-configuration","text":"In current implementation, Docker runner uses http_proxy and https_proxy environmental variables (if set) during configure and run phases: - configure : docker build ... --build-args http_proxy=${http_proxy} --build-args https_proxy=${https_proxy} ... - run : docker run ... -e http_proxy=${http_proxy} -e https_proxy=${https_proxy} ...","title":"Additional configuration"},{"location":"runners/docker-runner/#configuring-mlcubes","text":"Docker runner uses {MLCUBE_ROOT}/build directory as the build context directory. This implies that all files that must be packaged in a docker image, must be located in that directory, including source files, python requirements, resource files, ML models etc. The docker recipe must have the standard name Dockerfile . If Dockerfile file exists in {MLCUBE_ROOT}/build , the Docker runner assumes that it needs to build a docker image. If that file does not exists, the Docker runner will try to pull image with the specified name. Docker runner under the hood runs the following command line: cd {build_path}; docker build {env_args} -t {image_name} -f Dockerfile . where: - {build_path} is {MLCUBE_ROOT}/build root directory. - {env_args} is the arguments retrieved from user environment. Currently, only http_proxy and https_proxy are supported. - {image_name} is the image name defined in the platform configuration file. The configure command is optional and users do not necessarily need to be aware about it. The Docker runner auto-detects if docker image exists before running a task, and if it does not exist, the docker runner runs the configure command. During the configure phase, docker runner does not check if docker image exists. This means the following. If some of the implementation files have been modified, to rebuild the docker image users need to run the configure command explicitly.","title":"Configuring MLCubes"},{"location":"runners/docker-runner/#running-mlcubes","text":"Docker runner runs the following command: {docker_runtime} run --rm --net=host --privileged=true {volumes} {env_args} {image_name} {args} where: - {docker_exec} is the docker_runtime value from the Docker platform configuration file. - {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. - {env_args} is the arguments retrieved from user environment, currently, only http_proxy and https_proxy are supported. - {image_name} is the image name from the platform configuration file. - {args} is the task command line arguments, constructed automatically by the runner.","title":"Running MLCubes"},{"location":"runners/kubernetes/","text":"Kubernetes Runner \u00b6 The Kubernetes Runner runs a MLCube on a Kubernetes cluster. Skip over to the fun part. Why Kubernetes? \u00b6 One of the key goals of the MLCube project is to enable portability of ML models. Kubernetes offers the a good set of abstractions to enable model training to be portable across different compute platforms. Design \u00b6 Kubernetes Runner Proposal Doc The Kubernetes runner takes in a kubernetes specific task file in the run directory and re-uses the Docker runner platform config and prepares a Kubernetes Job manifest. The runner then creates the job on the Kubernetes cluster. The Kubernetes Runner takes in a MLCube run configuration file similar to other runners. With clear definitions of input and output bindings. Here's an example: schema_type : mlcube_invoke schema_version : 1.0.0 task_name : kubernetes # task name set to 'kubernetes' input_binding : # input parameters (name: value) data_dir : path : workspace/data k8s : pvc : mlcube-input ... output_binding : # output parameters (name: value) model_dir : path : workspace/model k8s : pvc : mlcube-output ... The Runner also re-uses the Docker platform config file. So it needs a Docker platform config file in the Box. Let's revisit the Docker platform config. schema_type : mlcube_platform schema_version : 0.1.0 platform : name : \"docker\" version : \">=18.01\" container : image : \"mlperf/mlcube:mnist\" With these two config files, the runner then constructs the following Kubernetes Job manifest. apiVersion : batch/v1 kind : Job metadata : namespace : default generateName : mlcube-mnist- spec : template : spec : containers : - name : mlcube-container image : mlperf/mlcube:mnist args : - --data_dir=/mnt/mlcube/mlcube-input/workspace/data - --model_dir=/mnt/mlcube/mlcube-output/workspace/model volumeMounts : - name : mlcube-input mountPath : /mnt/mlcube/mlcube-input - name : mlcube-output mountPath : /mnt/mlcube/mlcube-output volumes : - name : mlcube-input persistentVolumeClaim : claimName : mlcube-input - name : mlcube-output persistentVolumeClaim : claimName : mlcube-output restartPolicy : Never backoffLimit : 4 Configure a Box for the runner \u00b6 Prerequisites: A Kubernetes cluster KUBECONFIG for the cluster pre-created volumes for the Box Create a Task file \u00b6 Based on the setup, create a specific task file for the Box. Create a YAML file in the run directory \u00b6 touch run/kubernetes.yaml Set Schema for Task \u00b6 schema_type : mlcube_invoke schema_version : 1.0.0 Set task name \u00b6 task_name : kubernetes Set input and output bindings \u00b6 input_binding : # input parameters (name: value) data_dir : path : workspace/data k8s : pvc : mlcube-input output_binding : # output parameters (name: value) model_dir : path : workspace/model k8s : pvc : mlcube-output Run a cube with the CLI \u00b6 pip install mlcube-k8s mlcube_k8s run \\ --mlcube = examples/mnist \\ --platform = examples/mnist/platforms/docker.yaml \\ --task = examples/mnist/run/kubernetes.yaml \\ --loglevel INFO","title":"Kubernetes Runner"},{"location":"runners/kubernetes/#kubernetes-runner","text":"The Kubernetes Runner runs a MLCube on a Kubernetes cluster. Skip over to the fun part.","title":"Kubernetes Runner"},{"location":"runners/kubernetes/#why-kubernetes","text":"One of the key goals of the MLCube project is to enable portability of ML models. Kubernetes offers the a good set of abstractions to enable model training to be portable across different compute platforms.","title":"Why Kubernetes?"},{"location":"runners/kubernetes/#design","text":"Kubernetes Runner Proposal Doc The Kubernetes runner takes in a kubernetes specific task file in the run directory and re-uses the Docker runner platform config and prepares a Kubernetes Job manifest. The runner then creates the job on the Kubernetes cluster. The Kubernetes Runner takes in a MLCube run configuration file similar to other runners. With clear definitions of input and output bindings. Here's an example: schema_type : mlcube_invoke schema_version : 1.0.0 task_name : kubernetes # task name set to 'kubernetes' input_binding : # input parameters (name: value) data_dir : path : workspace/data k8s : pvc : mlcube-input ... output_binding : # output parameters (name: value) model_dir : path : workspace/model k8s : pvc : mlcube-output ... The Runner also re-uses the Docker platform config file. So it needs a Docker platform config file in the Box. Let's revisit the Docker platform config. schema_type : mlcube_platform schema_version : 0.1.0 platform : name : \"docker\" version : \">=18.01\" container : image : \"mlperf/mlcube:mnist\" With these two config files, the runner then constructs the following Kubernetes Job manifest. apiVersion : batch/v1 kind : Job metadata : namespace : default generateName : mlcube-mnist- spec : template : spec : containers : - name : mlcube-container image : mlperf/mlcube:mnist args : - --data_dir=/mnt/mlcube/mlcube-input/workspace/data - --model_dir=/mnt/mlcube/mlcube-output/workspace/model volumeMounts : - name : mlcube-input mountPath : /mnt/mlcube/mlcube-input - name : mlcube-output mountPath : /mnt/mlcube/mlcube-output volumes : - name : mlcube-input persistentVolumeClaim : claimName : mlcube-input - name : mlcube-output persistentVolumeClaim : claimName : mlcube-output restartPolicy : Never backoffLimit : 4","title":"Design"},{"location":"runners/kubernetes/#configure-a-box-for-the-runner","text":"Prerequisites: A Kubernetes cluster KUBECONFIG for the cluster pre-created volumes for the Box","title":"Configure a Box for the runner"},{"location":"runners/kubernetes/#create-a-task-file","text":"Based on the setup, create a specific task file for the Box.","title":"Create a Task file"},{"location":"runners/kubernetes/#create-a-yaml-file-in-the-run-directory","text":"touch run/kubernetes.yaml","title":"Create a YAML file in the run directory"},{"location":"runners/kubernetes/#set-schema-for-task","text":"schema_type : mlcube_invoke schema_version : 1.0.0","title":"Set Schema for Task"},{"location":"runners/kubernetes/#set-task-name","text":"task_name : kubernetes","title":"Set task name"},{"location":"runners/kubernetes/#set-input-and-output-bindings","text":"input_binding : # input parameters (name: value) data_dir : path : workspace/data k8s : pvc : mlcube-input output_binding : # output parameters (name: value) model_dir : path : workspace/model k8s : pvc : mlcube-output","title":"Set input and output bindings"},{"location":"runners/kubernetes/#run-a-cube-with-the-cli","text":"pip install mlcube-k8s mlcube_k8s run \\ --mlcube = examples/mnist \\ --platform = examples/mnist/platforms/docker.yaml \\ --task = examples/mnist/run/kubernetes.yaml \\ --loglevel INFO","title":"Run a cube with the CLI"},{"location":"runners/singularity-runner/","text":"Singularity Runner \u00b6 Singularity runner uses singularity to run MLCommon-Box cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Singularity platform configuration is used to configure Singularity runner. Platform Configuration File \u00b6 Singularity platform configuration file is a YAML file that follows mlcube_singularity ML schema. The configuration file for the reference MNIST cube is the following: schema_version : 1.0.0 schema_type : mlcube_singularity image : /opt/singularity/mlperf_mlcube_mnist-0.01.simg # Path to or name of a Singularity image. The image field above is a path to a singularity container. It is relative to {MLCUBE_ROOT}/workspace : - By default, containers are stored in {MLCUBE_ROOT}/workspace if image is a file name. - If it is a relative path, it is relative to {MLCUBE_ROOT}/workspace . - Absolute paths (starting with /) are used as is. In the example above, Singularity image is stored in the directory outside of the {MLCUBE_ROOT} to avoid copying it back to a user host when using runners such as SSH. Build command \u00b6 Singularity runner uses {MLCUBE_ROOT}/build directory as the build context directory. This implies that all files that must be packaged in a singularity image, must be located in that directory, including source files, python requirements, resource files, ML models etc. The singularity recipe must have the standard name Singularity.recipe . Singularity runner under the hood runs the following command line: cd {build_path}; singularity build --fakeroot {image_path} Singularity.recipe where: - {build_path} is {MLCUBE_ROOT}/build root directory. - {image_path} is the path to Singularity image that is computed as described above. Run command \u00b6 Singularity runner runs the following command: singularity run {volumes} {image_path} {args} where: - {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. - {image_path} is the path to Singularity image that is computed as described above. - {args} is the task command line arguments, constructed automatically by the runner.","title":"Singularity Runner"},{"location":"runners/singularity-runner/#singularity-runner","text":"Singularity runner uses singularity to run MLCommon-Box cubes. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . Singularity platform configuration is used to configure Singularity runner.","title":"Singularity Runner"},{"location":"runners/singularity-runner/#platform-configuration-file","text":"Singularity platform configuration file is a YAML file that follows mlcube_singularity ML schema. The configuration file for the reference MNIST cube is the following: schema_version : 1.0.0 schema_type : mlcube_singularity image : /opt/singularity/mlperf_mlcube_mnist-0.01.simg # Path to or name of a Singularity image. The image field above is a path to a singularity container. It is relative to {MLCUBE_ROOT}/workspace : - By default, containers are stored in {MLCUBE_ROOT}/workspace if image is a file name. - If it is a relative path, it is relative to {MLCUBE_ROOT}/workspace . - Absolute paths (starting with /) are used as is. In the example above, Singularity image is stored in the directory outside of the {MLCUBE_ROOT} to avoid copying it back to a user host when using runners such as SSH.","title":"Platform Configuration File"},{"location":"runners/singularity-runner/#build-command","text":"Singularity runner uses {MLCUBE_ROOT}/build directory as the build context directory. This implies that all files that must be packaged in a singularity image, must be located in that directory, including source files, python requirements, resource files, ML models etc. The singularity recipe must have the standard name Singularity.recipe . Singularity runner under the hood runs the following command line: cd {build_path}; singularity build --fakeroot {image_path} Singularity.recipe where: - {build_path} is {MLCUBE_ROOT}/build root directory. - {image_path} is the path to Singularity image that is computed as described above.","title":"Build command"},{"location":"runners/singularity-runner/#run-command","text":"Singularity runner runs the following command: singularity run {volumes} {image_path} {args} where: - {volumes} are the mount points that the runner automatically constructs based upon the task input/output specifications. - {image_path} is the path to Singularity image that is computed as described above. - {args} is the task command line arguments, constructed automatically by the runner.","title":"Run command"},{"location":"runners/ssh-runner/","text":"SSH Runner \u00b6 SSH runner uses other runners to run MLCube cubes on remote hosts. It uses ssh and rsync internally. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . SSH platform configuration is used to configure SSH runner. This runner is being actively developed and not all features described on this page may be supported. Platform Configuration File \u00b6 SSH platform configuration file is a YAML file. The configuration file for the reference MNIST cube is the following: # Possible values are hostname or IP address. It can also be a host alias if there is a corresponding section # exists in ~/.ssh/config. The hostname should be possible to use with tools like ssh, rsync and scp. host : REMOTE_HOST # Authentication section is optional, and can be null, empty or non-empty dictionary. If value is null or empty # string/dict, it is assumed the authentication is not required, or is configured in user environment # (e.g. ~/.ssh/config). SSH runner will not provide any additional information on a command line for ssh, rsync or scp. # If the value is a dictionary, optional fields that SSH runner recognizes are user name on a remote host (user) and # path to a user private key file (identity_file). If all fields are present, the following connection string is used # by the SSH runner: `-i $identity_file $user@$host`. authentication : user : USER identity_file : /opt/mlcube/ssh/gcp_identity # The platform field points to a platform configuration to be used on a remote host. This file must be located inside # $mlcube_root/platforms directory. The idea is that the SSH runner delivers an MLCube to a remote host and then use # another MLCube runner such as Docker or Singularity runner to run that MLCube there. platform : docker.yaml # The interpreter section defines python interpreter on a remote host to use to run other runners there. This is not # environment for MLCubes, this is environment for runners to run MLCubes. Two options are supported - `system` and # `virtualenv` interpreters. # The system interpreter is a python already available on a remote host. It can be just an executable (python, # python3.8) or a full path to a user existing environment. Three fields should be provided: type (system in this case), # python executable, possibly, with fully specified path, and dependencies in the form of a string (requirements). interpreter : type : \"system\" python : \"python3.6\" requirements : \"mlcube-docker==0.2.2\" # The virtualenv interpreter does not have to exist. SSH runner can create this one. This interpreter has the same # fields with two additional ones - location (base path for a python environment) and name (basically, a folder name # inside the location path). # interpreter: # type: \"virtualenv\" # python: \"python3.6\" # requirements: \"mlcube-docker==0.2.1\" # location: \"${HOME}/mlcube/environments\" # name: \"mlcube-docker-0.2.1\" SSH runner uses IP or name of a remote host ( host ) and ssh tool to login and execute shell commands on remote hosts. If passwordless login is not configured, SSH runner asks for password many times during configure and run phases. SSH runner depends on other runners to run MLCube cubes. The platform field specifies what runner should be used on a remote host. This is a file name located in {MLCUBE_ROOT}/platforms . In current implementation, SSH runner synchronizes only an mlcube workload between local and remote hosts. Runners are assumed to be either available on remote hosts or specified as package dependencies in python interpreter configuration section. Configure command \u00b6 During the build phase, the following steps are performed. 1. Based upon configuration, SSH runner creates and/or configures python on a remote host using ssh . This includes execution of such commands as virtualenv -p ... and/or source ... && pip install ... on a remote host. Default path for a python environment on a remote host is ${HOME}/mlcube/environments/ . 2. SSH runner copies mlcube directory to a remote host. Default path on a remote host is ${HOME}/mlcube/cubes/ . 3. SSH runner runs another runner specified in a platform configuration file on a remote host to configure it. Run command \u00b6 During the run phase, the SSH runner performs the following steps: 1. It uses ssh to run standard run command on a remote host. 2. It uses rsync to synchronize back the content of the {MLCUBE_ROOT}/workspace directory.","title":"SSH Runner"},{"location":"runners/ssh-runner/#ssh-runner","text":"SSH runner uses other runners to run MLCube cubes on remote hosts. It uses ssh and rsync internally. It supports two mandatory commands - configure and run with standard arguments - mlcube , platform and task . SSH platform configuration is used to configure SSH runner. This runner is being actively developed and not all features described on this page may be supported.","title":"SSH Runner"},{"location":"runners/ssh-runner/#platform-configuration-file","text":"SSH platform configuration file is a YAML file. The configuration file for the reference MNIST cube is the following: # Possible values are hostname or IP address. It can also be a host alias if there is a corresponding section # exists in ~/.ssh/config. The hostname should be possible to use with tools like ssh, rsync and scp. host : REMOTE_HOST # Authentication section is optional, and can be null, empty or non-empty dictionary. If value is null or empty # string/dict, it is assumed the authentication is not required, or is configured in user environment # (e.g. ~/.ssh/config). SSH runner will not provide any additional information on a command line for ssh, rsync or scp. # If the value is a dictionary, optional fields that SSH runner recognizes are user name on a remote host (user) and # path to a user private key file (identity_file). If all fields are present, the following connection string is used # by the SSH runner: `-i $identity_file $user@$host`. authentication : user : USER identity_file : /opt/mlcube/ssh/gcp_identity # The platform field points to a platform configuration to be used on a remote host. This file must be located inside # $mlcube_root/platforms directory. The idea is that the SSH runner delivers an MLCube to a remote host and then use # another MLCube runner such as Docker or Singularity runner to run that MLCube there. platform : docker.yaml # The interpreter section defines python interpreter on a remote host to use to run other runners there. This is not # environment for MLCubes, this is environment for runners to run MLCubes. Two options are supported - `system` and # `virtualenv` interpreters. # The system interpreter is a python already available on a remote host. It can be just an executable (python, # python3.8) or a full path to a user existing environment. Three fields should be provided: type (system in this case), # python executable, possibly, with fully specified path, and dependencies in the form of a string (requirements). interpreter : type : \"system\" python : \"python3.6\" requirements : \"mlcube-docker==0.2.2\" # The virtualenv interpreter does not have to exist. SSH runner can create this one. This interpreter has the same # fields with two additional ones - location (base path for a python environment) and name (basically, a folder name # inside the location path). # interpreter: # type: \"virtualenv\" # python: \"python3.6\" # requirements: \"mlcube-docker==0.2.1\" # location: \"${HOME}/mlcube/environments\" # name: \"mlcube-docker-0.2.1\" SSH runner uses IP or name of a remote host ( host ) and ssh tool to login and execute shell commands on remote hosts. If passwordless login is not configured, SSH runner asks for password many times during configure and run phases. SSH runner depends on other runners to run MLCube cubes. The platform field specifies what runner should be used on a remote host. This is a file name located in {MLCUBE_ROOT}/platforms . In current implementation, SSH runner synchronizes only an mlcube workload between local and remote hosts. Runners are assumed to be either available on remote hosts or specified as package dependencies in python interpreter configuration section.","title":"Platform Configuration File"},{"location":"runners/ssh-runner/#configure-command","text":"During the build phase, the following steps are performed. 1. Based upon configuration, SSH runner creates and/or configures python on a remote host using ssh . This includes execution of such commands as virtualenv -p ... and/or source ... && pip install ... on a remote host. Default path for a python environment on a remote host is ${HOME}/mlcube/environments/ . 2. SSH runner copies mlcube directory to a remote host. Default path on a remote host is ${HOME}/mlcube/cubes/ . 3. SSH runner runs another runner specified in a platform configuration file on a remote host to configure it.","title":"Configure command"},{"location":"runners/ssh-runner/#run-command","text":"During the run phase, the SSH runner performs the following steps: 1. It uses ssh to run standard run command on a remote host. 2. It uses rsync to synchronize back the content of the {MLCUBE_ROOT}/workspace directory.","title":"Run command"}]}